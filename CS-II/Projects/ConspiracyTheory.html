<!DOCTYPE html>
<html lang="en">
  <head>
    <link rel="stylesheet" href="../../styles.css">
    <meta charset="utf-8">
    <title>Conspiracy Theories</title>
  </head>
  <body>
    <div>
      <h1>Algorithm Awareness & The Spread of Conspiracy Theories</h1>
      <div class="sub">
	<ul>
	  <li><h3>The Basics of Recommendation Algorithms</h3></li>
	  <p> Nearly every digital platform relies on recommendation algorithms in order to increase user interest, drive more engagement, and ultimately more commercial success for the platform. Instead of chronological or even randomly based recommendations, algorithms collect data on individual users, such as interactions and time viewing, to run through various calculations and output more content that the user is more likely to enjoy. The algorithms can even use external data from other users with similar interests to predict thing new enjoyable topics that the a user hasn't even discovered yet.  </p>
	  <li><h3>The Feedback Loop</h3></li>
	  <p> As the user interacts with the platform for longer periods of time, more content can be displayed and the dataset for the algorithm grows. Profiles grow around individual users and each of their personal experiences may differ vastly from each other. If a recommendation algorith does its job properly, use of a platform enables and encourages further use. A side effect of this system is that the algorithm almost unavoidably weeds out bland or mundane content, favoring instead more opinionated or extreme pieces instead as they usually enable more engagement. In some cases, the algorithm may accidentally create "filter bubbles" as the user interacts with only topics or ideas they agree with. This can create instances where users form a skewed idea of a topic as they are only exposed to a singular facet of an argument or opinion. </p>
	  <li><h3>Conspiracy Theories & Algorithmic Amplification</h3></li>
	  <p> Conspiracy theories commonly gain traction on platforms with recommendation algorithms due to their controversial natures, facilitating angered or emotional responses. Algorithms may mistake these negative engagements as shows of approval by users and decide to continue pushing them to larger audiences. The more a person interacts or entertains a theory, even if in a negative light, the more the algorithm amplifies it, possibly spreading the idea to more persuadable persons and giving attention to the entity or being who started it. </p>
	  <li><h3>Platform Incentives</h3></li>
	  <p> Morally, solving and avoiding these issues would be a top priorty for digital platforms, but unfortunatly morals is not all at stake. In the end, even if the content isn't very palatable, the recommendation algorithm has only succeeded in its goal of growing engagement and increasing profits. If a user becomes reliant on a platform to receive validation for their extreme ideas or spreading negative statements on other pieces of content, this gives them free rein to monetize and exploit this user with advertisements and/or paywalls. The most a platform may want to do is save face by hiding these people and engagements from the general public as to avoid bad press, but by no means does it benefit a platform to eradicate the emotional exploits unless completely necessary.</p>
	  <li><h3>Potential for Exploitation</h3></li>
	  <p> Now, more and more are becoming aware of this fault on digital platforms, but drastically more vulnerable people are joining the platforms simultaneously. Starting with "clickbait," digital content has evolved to abuse these exploits, now even to harmful and malicious extents. Lying and misrepresenting truth has become unprecedentally profitable compared to their previous counterparts. Entire platforms and organizations are now built around creating provacative movements and content, whether representing their true beliefs or not, in order to farm engagement and money from the vulnerable </p>
	</ul>
      </div>
  </body>
</html>
